{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune. In this notebook, you'll be using VGGNet trained on the ImageNet dataset as a feature extractor. Below is a diagram of the VGGNet architecture.\n",
    "\n",
    "VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but replace the final fully connected layers with our own classifier. This way we can use VGGNet as a feature extractor for our images then easily train a simple classifier on top of that. What we'll do is take the first fully connected layer with 4096 units, including thresholding with ReLUs. We can use those values as a code for each image, then build a classifier on top of those codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a pretrained network from https://github.com/machrisaa/tensorflow-vgg.\n",
    "\n",
    "This is a really nice implementation of VGGNet, quite easy to work with. The network has already been trained and the parameters are available from this link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VGG16 Parameters: 553MB [28:38, 322kB/s]                                                                               \n"
     ]
    }
   ],
   "source": [
    "vgg_dir = 'tensorflow_vgg/'\n",
    "if not isdir(vgg_dir):\n",
    "    raise Exception(\"VGG directory doesn't exists\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    \n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "    \n",
    "if not isfile(vgg_dir + 'vgg15.npy'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='VGG16 Parameters') as pbar:\n",
    "        urlretrieve('https://s3.amazonaws.com/content.udacity-data.com/nd101/vgg16.npy',\n",
    "                    vgg_dir + 'vgg16.npy', pbar.hook)\n",
    "else:\n",
    "    print('Parameters file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flowers Dataset: 229MB [02:23, 1.60MB/s]                                                                               \n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "dataset_folder_path = 'flower_photos'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('flower_photos.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "            'flower_photos.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(dataset_folder_path):\n",
    "    with tarfile.open('flower_photos.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snettani\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flower_photos/'\n",
    "contents = os.listdir(data_dir)\n",
    "classes = [cls for cls in contents if os.path.isdir(data_dir+cls)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the images through the VGG network in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snettani\\Documents\\Personal\\Github\\Udacity_DeepLearning\\TransferLearning\\tensorflow_vgg\\vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 4s\n",
      "starting daisy images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snettani\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "starting dandelion images\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "starting roses images\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "starting sunflowers images\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "starting tulips images\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n",
      "images processed\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "codes_list = []\n",
    "labels = []\n",
    "batch = []\n",
    "\n",
    "codes = None\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    vgg = vgg16.Vgg16()\n",
    "    input_ = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    with tf.name_scope('content_vgg'):\n",
    "        vgg.build(input_)\n",
    "    \n",
    "    for cls in classes:\n",
    "        print('starting {} images'.format(cls))\n",
    "        class_path = data_dir + cls\n",
    "        files = os.listdir(class_path)\n",
    "        for i, file in enumerate(files, 1):\n",
    "            #add images to the current batch\n",
    "            #utils.load_image crops the images for us, from the center\n",
    "            img = utils.load_image(os.path.join(class_path, file))\n",
    "            batch.append(img.reshape((1, 224, 224, 3)))\n",
    "            labels.append(cls)\n",
    "            \n",
    "            #running the batch through the network to get the codes\n",
    "            if i%batch_size == 0 or i == len(files):\n",
    "                images = np.concatenate(batch)\n",
    "                feed_dict = {input_:images}\n",
    "                codes_batch = sess.run(vgg.relu6, feed_dict=feed_dict)\n",
    "                \n",
    "                #Build an array of codes\n",
    "                if codes is None:\n",
    "                    codes = codes_batch\n",
    "                else:\n",
    "                    codes = np.concatenate((codes, codes_batch))\n",
    "                \n",
    "                #reset to start building the next bacth\n",
    "                batch = []\n",
    "                print('images processed'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write codes to the file\n",
    "with open('codes', 'w') as fp:\n",
    "    codes.tofile(fp)\n",
    "\n",
    "#write labels to the file\n",
    "import csv\n",
    "with open('labels', 'w') as fp:\n",
    "    writer = csv.writer(fp, delimiter='\\n')\n",
    "    writer.writerow(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier\n",
    "\n",
    "Now that we have codes for all the images, we can build a simple classifier on top of them. The codes behave just like normal input into a simple neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read codes and labels from the file\n",
    "with open('codes') as fp:\n",
    "    codes = np.fromfile(fp, dtype=np.float32)\n",
    "    codes = codes.reshape((len(labels), -1))\n",
    "\n",
    "    \n",
    "with open('labels') as fp:\n",
    "    reader = csv.reader(fp, delimiter='\\n')\n",
    "    labels = np.array([each for each in reader if len(each) > 0]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep\n",
    "\n",
    "As usual, now we need to one-hot encode our labels and create validation/test sets. First up, creating our labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "labels_vecs = lb.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll want to create your training, validation, and test sets. An important thing to note here is that our labels and data aren't randomized yet. We'll want to shuffle our data so the validation and test sets contain data from all classes. Otherwise, you could end up with testing sets that are all one class. Typically, you'll also want to make sure that each smaller set has the same the distribution of classes as it is for the whole data set. The easiest way to accomplish both these goals is to use StratifiedShuffleSplit from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "train_idx, val_idx = next(ss.split(codes, labels_vecs))\n",
    "half_val_len = int(len(val_idx)/2)\n",
    "val_idx, test_idx = val_idx[:half_val_len], val_idx[half_val_len:]\n",
    "\n",
    "train_x, train_y = codes[train_idx], labels_vecs[train_idx]\n",
    "val_x, val_y = codes[val_idx], labels_vecs[val_idx]\n",
    "test_x, test_y = codes[test_idx], labels_vecs[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes (x, y): (2936, 4096) (2936, 5)\n",
      "Validation shapes (x, y): (367, 4096) (367, 5)\n",
      "Test shapes (x, y): (367, 4096) (367, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shapes (x, y):\", train_x.shape, train_y.shape)\n",
    "print(\"Validation shapes (x, y):\", val_x.shape, val_y.shape)\n",
    "print(\"Test shapes (x, y):\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier layers\n",
    "\n",
    "Once you have the convolutional codes, you just need to build a classfier from some fully connected layers. You use the codes as the inputs and the image labels as targets. Otherwise the classifier is a typical neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
